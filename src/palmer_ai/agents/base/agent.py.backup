"""Base Agent Architecture with UWAS Integration"""
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional, TypeVar, Generic
from pydantic import BaseModel, Field
from datetime import datetime
import asyncio
from enum import Enum

from ...utils.logger import get_logger
from ...frameworks.uwas.reasoning import UWASReasoning

logger = get_logger(__name__)

T = TypeVar('T')

class AgentRole(str, Enum):
    RECONNAISSANCE = "reconnaissance"
    COMPETITIVE_INTELLIGENCE = "competitive_intelligence"
    UX_EVALUATION = "ux_evaluation"
    VISUAL_INTELLIGENCE = "visual_intelligence"
    TECHNICAL_PERFORMANCE = "technical_performance"
    STRATEGIC_SYNTHESIS = "strategic_synthesis"
    SUPERVISOR = "supervisor"

class ConfidenceLevel(str, Enum):
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    
class AgentCapability(BaseModel):
    name: str
    description: str
    required_resources: List[str] = Field(default_factory=list)
    performance_metrics: Dict[str, float] = Field(default_factory=dict)

class AgentConfig(BaseModel):
    agent_id: str
    name: str
    role: AgentRole
    description: str
    capabilities: List[AgentCapability]
    model_config: Dict[str, Any] = Field(default_factory=dict)
    max_concurrent_tasks: int = 5
    timeout_seconds: int = 300
    uwas_techniques: List[str] = Field(default_factory=list)
    
class AgentMessage(BaseModel):
    sender: str
    recipient: str
    message_type: str
    content: Dict[str, Any]
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    correlation_id: Optional[str] = None
    
class AnalysisResult(BaseModel, Generic[T]):
    success: bool
    data: Optional[T] = None
    errors: List[str] = Field(default_factory=list)
    warnings: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    confidence: ConfidenceLevel
    evidence_trail: List[Dict[str, Any]] = Field(default_factory=list)
    reasoning_path: Optional[str] = None
    
class BaseAgent(ABC):
    """Base class for all Palmer AI agents with UWAS reasoning capabilities"""
    
    def __init__(self, config: AgentConfig):
        self.config = config
        self.logger = get_logger(f"{__name__}.{config.agent_id}")
        self.uwas_reasoning = UWASReasoning(config.uwas_techniques)
        self._message_queue: asyncio.Queue = asyncio.Queue()
        self._active_tasks: Dict[str, asyncio.Task] = {}
        self._performance_metrics: Dict[str, List[float]] = {}
        
    @abstractmethod
    async def analyze(self, input_data: Dict[str, Any]) -> AnalysisResult:
        """Perform agent-specific analysis"""
        pass
        
    @abstractmethod
    async def collaborate(self, message: AgentMessage) -> Optional[AgentMessage]:
        """Handle collaboration messages from other agents"""
        pass
        
    async def process_with_uwas(self, 
                                task: str, 
                                data: Dict[str, Any],
                                techniques: Optional[List[str]] = None) -> Dict[str, Any]:
        """Apply UWAS reasoning techniques to analysis task"""
        techniques = techniques or self.config.uwas_techniques
        return await self.uwas_reasoning.apply_techniques(task, data, techniques)
        
    async def send_message(self, recipient: str, message_type: str, content: Dict[str, Any]):
        """Send message to another agent"""
        message = AgentMessage(
            sender=self.config.agent_id,
            recipient=recipient,
            message_type=message_type,
            content=content
        )
        # Implementation will connect to message broker
        self.logger.info(f"Sending message to {recipient}: {message_type}")
        
    async def calculate_confidence(self, evidence: List[Dict[str, Any]]) -> ConfidenceLevel:
        """Calculate confidence level based on evidence quality"""
        if not evidence:
            return ConfidenceLevel.LOW
            
        quality_scores = []
        for item in evidence:
            score = item.get("quality_score", 0.5)
            weight = item.get("weight", 1.0)
            quality_scores.append(score * weight)
            
        avg_score = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        if avg_score >= 0.8:
            return ConfidenceLevel.HIGH
        elif avg_score >= 0.5:
            return ConfidenceLevel.MEDIUM
        else:
            return ConfidenceLevel.LOW
            
    async def validate_input(self, input_data: Dict[str, Any]) -> List[str]:
        """Validate input data for agent processing"""
        errors = []
        # Override in subclasses for specific validation
        return errors
        
    def record_performance(self, metric: str, value: float):
        """Record performance metrics for monitoring"""
        if metric not in self._performance_metrics:
            self._performance_metrics[metric] = []
        self._performance_metrics[metric].append(value)
        
    async def get_performance_summary(self) -> Dict[str, Dict[str, float]]:
        """Get summary of agent performance metrics"""
        summary = {}
        for metric, values in self._performance_metrics.items():
            if values:
                summary[metric] = {
                    "mean": sum(values) / len(values),
                    "min": min(values),
                    "max": max(values),
                    "count": len(values)
                }
        return summary
